{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ef77145fe04ea1a56b8a6a50502a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c6eb12fe31429e8d26449dad7a2b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Rapidata/text-2-image-Rich-Human-Feedback\", split=\"train\", streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "def pil_to_base64(image: Image.Image) -> str:\n",
    "    buffer = BytesIO()\n",
    "    image.save('output.jpg', format=\"JPEG\")\n",
    "    image.save(buffer, format=\"JPEG\")  # Ensure the format is set to JPEG\n",
    "    base64_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return base64_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_words = ds.select_columns([\"image\", \"prompt\", \"alignment_score\", \"coherence_score\", \"word_scores\"])\n",
    "for example in ds_words.take(500):\n",
    "    if example[\"alignment_score\"]  < 2.5:\n",
    "        result = example\n",
    "        print(example[\"alignment_score\"], example[\"prompt\"])\n",
    "        pil_to_base64(example[\"image\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"A\", 0.9753], [\"sign\", 2.6966], [\"that\", 0.9753], [\"says\", 1.8856], [\"'Diffusion'.\", 8.1711], [\"[No_mistakes]\", 10.2536]]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"word_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# Assuming you have an IterableDataset\n",
    "dataset = ds_words\n",
    "\n",
    "# Get the 3rd element (0-based index, so index 2)\n",
    "third_element = next(islice(dataset, 63, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1654999256134033"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_element['alignment_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a landscaping company called norwegian fjord'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_element['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_image = pil_to_base64(third_element[\"image\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:28,  1.76it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fe751dbabc0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luckyman/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "71it [00:41,  2.09it/s]"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "from tqdm import tqdm\n",
    "for example in tqdm(ds_words):\n",
    "    scores.append(example['alignment_score'])\n",
    "\n",
    "import numpy as np\n",
    "print(np.min(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess image\n",
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load CLIP model\n",
    "\n",
    "def clip_score(example):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    image = preprocess(example[\"image\"]).unsqueeze(0)\n",
    "\n",
    "    # Tokenize text\n",
    "    text = tokenizer(example[\"prompt\"])\n",
    "\n",
    "    # Move to available device (CPU/GPU)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    text = text.to(device)\n",
    "\n",
    "    # Compute embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "    # Normalize features\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = 4*((image_features @ text_features.T).item())+1\n",
    "\n",
    "    print(f\"Similarity Score: {similarity:.4f}\")\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luckyman/.local/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 2.0833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0832602977752686"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_score(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(llm_score(element=result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luckyman/.local/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 2.0694\n",
      "Similarity Score: 2.3566\n",
      "Similarity Score: 2.3202\n",
      "Similarity Score: 2.0886\n",
      "Similarity Score: 2.3886\n",
      "Similarity Score: 2.2289\n",
      "Similarity Score: 2.5424\n",
      "Similarity Score: 2.3901\n",
      "Similarity Score: 2.1493\n",
      "Similarity Score: 2.2739\n"
     ]
    }
   ],
   "source": [
    "clip_metric = []\n",
    "human_metric = []\n",
    "\n",
    "for element in islice(dataset, 0, 10):\n",
    "    base64_image = pil_to_base64(element[\"image\"])\n",
    "    clip_metric.append(clip_score(element[\"prompt\"]))\n",
    "    human_metric.append(element[\"alignment_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1024 at 0x7FCD6D3BF970>, 'prompt': 'The bright green grass contrasted with the dull grey pavement.', 'alignment_score': 3.45740008354187, 'coherence_score': 3.596299886703491}\n"
     ]
    }
   ],
   "source": [
    "elements = next(islice(dataset, 0, 10))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt_template = \"\"\"\n",
    "Task description:\n",
    "In this task you will evaluate 3 metrics of image generated by AI.\n",
    "Metrics: Plausability of image, Text-Alignment(overall alignment image to promp), Aesthetics \n",
    "To correctly make evaluation follow the INSTRUCTIONS\n",
    "**INSTRUCTIONS**:\n",
    "1. Understand the Textual Prompt\n",
    "    a. Extract the core meaning of the prompt.\n",
    "    b. Identify key elements such as objects, actions, relationships, emotions, colors, styles, or any specific details mentioned.\n",
    "2. Analyze the Image\n",
    "    a. Identify the main subjects, objects, actions, and visual characteristics of the image.\n",
    "    b. Consider elements such as composition, colors, textures, and details that align with the prompt.\n",
    "3. Rate image in 3 metrics based on evaluation criteria(0 worst, 1 best)\n",
    "\n",
    "**Evaluation critera**\n",
    "#### Evaluation criteria for Text Alignemt\n",
    "1. Check if the main objects and actions in the prompt appear in the image.\n",
    "2. Assess semantic alignment (e.g., if the prompt describes a \"red apple,\" ensure a red apple is present rather than a green one).\n",
    "3. Consider stylistic and contextual alignment (e.g., if the prompt asks for \"a realistic portrait,\" but the image is abstract, the alignment is lower).\n",
    "\n",
    "1.0 → Perfect match (The image fully matches the prompt in all aspects).\n",
    "0.8 - 0.9 → High alignment (Minor deviations in details but the overall meaning is correct).\n",
    "0.5 - 0.7 → Partial match (Some elements are missing or incorrect, but the general idea is preserved).\n",
    "0.2 - 0.4 → Low alignment (The image and prompt share few relevant elements).\n",
    "0.0 - 0.1 → No alignment (The image does not represent the prompt at all).\n",
    "\n",
    "#### Evaluation criteria for Plausability\n",
    "1. Check if image looks natural and free from visible artifacts\n",
    "2. Check if elements on image are logically aligned\n",
    "3. Are textures, facial features, and small elements consistent and believable?\n",
    "\n",
    "1.0 → Perfect match (The image looks really natural without any visible artifacts).\n",
    "0.8 - 0.9 → High alignment (Image has small artifact or some objects are misaligned).\n",
    "0.5 - 0.7 → Partial match (The image has noticeable artifacts or unnatural elements.).\n",
    "0.2 - 0.4 → Low alignment (The image has multiple artifacts or unnatural distortions.).\n",
    "0.0 - 0.1 → No alignment (The image is heavily distorted or unnatural.).\n",
    "\n",
    "#### Evaluation criteria for Aesthetics\n",
    "1. Check overall image sharpness and quality\n",
    "2. Analyze color palette cohesiveness and appropriateness\n",
    "3. Check how well looking is image at all\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.046199798583984 4.280099868774414\n"
     ]
    }
   ],
   "source": [
    "print(third_element['alignment_score'], third_element['coherence_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The bright green grass contrasted with the dull grey pavement.\n",
      "4.6\n",
      "1 image from an iPhone video of a dog in a supermarket, hyper realistic, flash photo\n",
      "5.0\n",
      "2 A man wearing a brown cap looking sitting at his computer with a black and brown dog resting next to him on the couch.\n",
      "5.0\n",
      "3 A beige pastry sitting in a white ball next to a spoon .\n",
      "4.2\n",
      "4 a diverse crowd of people eagerly waits in line at a bustling street food stand in beirut. the tantalizing aroma of freshly grilled kebabs and warm pita bread fills the air. the stand is adorned with vibrant lanterns and colorful signs showcasing the mouthwatering menu. nearby, a street musician plays an upbeat tune on his oud, adding to the lively atmosphere.\n",
      "4.6\n",
      "5 photograph of a person drinking red wine and smoking weed with a flat cigarette\n",
      "5.0\n",
      "6 A yellow horse and a red chair.\n",
      "5.0\n",
      "7 A guitar made of ice cream that melts as you play it.\n",
      "5.0\n",
      "8 a fluffy pillow and a leather belt\n",
      "5.0\n",
      "9 hyperrealism fruits and vegetables market\n",
      "5.0\n",
      "0.34191842968811503\n"
     ]
    }
   ],
   "source": [
    "llm_metric = []\n",
    "human_metric = []\n",
    "\n",
    "for idx, element in enumerate(islice(dataset, 0, 10)):\n",
    "    base64_image = pil_to_base64(element[\"image\"])\n",
    "    print(idx, element['prompt'])\n",
    "    score = float(llm_score(element=element))\n",
    "    \n",
    "    score = 4*score+1\n",
    "    print(score)\n",
    "    llm_metric.append(float(score))\n",
    "    human_metric.append(element[\"alignment_score\"])\n",
    "\n",
    "import scipy.stats as stats\n",
    "kendall_tau, _ = stats.kendalltau(llm_metric, human_metric)\n",
    "print(kendall_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.45740008354187,\n",
       " 3.4003000259399414,\n",
       " 4.132400035858154,\n",
       " 2.8675999641418457,\n",
       " 3.7504000663757324,\n",
       " 3.6187000274658203,\n",
       " 3.5425000190734863,\n",
       " 3.8287999629974365,\n",
       " 3.3171000480651855,\n",
       " 3.9697999954223633]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.6, 5.0, 5.0, 4.2, 4.6, 5.0, 5.0, 5.0, 5.0, 5.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "kendall_tau, _ = stats.kendalltau(clip_metric, human_metric)\n",
    "print(kendall_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_version_prompt = \"\"\" You are helpful assistant. You will be provided with description and image. Your goal is to determine how good image align with the description. \n",
    "                            Follow instructions step by step and verify each step.\n",
    "                            Instruction:\n",
    "                            Steps to Evaluate Alignment\n",
    "\n",
    "                            1. Understand the Textual Prompt\n",
    "                                a. Extract the core meaning of the prompt.\n",
    "                                b. Identify key elements such as objects, actions, relationships, emotions, colors, styles, or any specific details mentioned.\n",
    "\n",
    "                            2. Analyze the Image\n",
    "                                a. Identify the main subjects, objects, actions, and visual characteristics of the image.\n",
    "                                b. Consider elements such as composition, colors, textures, and details that align with the prompt.\n",
    "\n",
    "                            3. Compare Textual and Visual Features\n",
    "                                a. Check if the main objects and actions in the prompt appear in the image.\n",
    "                                b. Assess semantic alignment (e.g., if the prompt describes a \"red apple,\" ensure a red apple is present rather than a green one).\n",
    "                                c. Consider stylistic and contextual alignment (e.g., if the prompt asks for \"a realistic portrait,\" but the image is abstract, the alignment is lower).\n",
    "\n",
    "                            4. Assign an Alignment Score (0-1 Scale)\n",
    "                                1.0 → Perfect match (The image fully matches the prompt in all aspects).\n",
    "                                0.8 - 0.9 → High alignment (Minor deviations in details but the overall meaning is correct).\n",
    "                                0.5 - 0.7 → Partial match (Some elements are missing or incorrect, but the general idea is preserved).\n",
    "                                0.2 - 0.4 → Low alignment (The image and prompt share few relevant elements).\n",
    "                                0.0 - 0.1 → No alignment (The image does not represent the prompt at all).\n",
    "                            5. As response return ONLY Alignment score as number without any additional thoughts and comments\n",
    "                            \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
